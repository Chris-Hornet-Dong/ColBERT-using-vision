import os
import glob
# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œè®©æ¨¡å‹ä¸‹è½½åˆ°Dç›˜
os.environ['HF_HOME'] = 'D:/huggingface_cache/models'
os.environ['TRANSFORMERS_CACHE'] = 'D:/huggingface_cache/transformers'
os.environ['HF_HUB_CACHE'] = 'D:/huggingface_cache/hub'
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'
os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '1000'

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers.models.clip import CLIPProcessor, CLIPModel
from transformers.models.auto.modeling_auto import AutoModel
from transformers.models.auto.tokenization_auto import AutoTokenizer
from PIL import Image



class VisionColBERT(nn.Module):
    
    def __init__(self, 
                 colbert_model_name="colbert-ir/colbertv2.0",
                 clip_model_name="openai/clip-vit-base-patch16",
                 output_dim=256):

        super().__init__()
        
        # åŠ è½½ColBERTæ¨¡å‹å’Œåˆ†è¯å™¨
        self.colbert = AutoModel.from_pretrained(colbert_model_name)
        self.colbert_tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")  # ColBERTä½¿ç”¨BERTåˆ†è¯å™¨
        
        # åŠ è½½CLIPæ¨¡å‹å’Œå¤„ç†å™¨
        self.clip = CLIPModel.from_pretrained(clip_model_name)
        self.clip_processor = CLIPProcessor.from_pretrained(clip_model_name)
        
        # è·å–ç»´åº¦ä¿¡æ¯
        colbert_hidden_size = self.colbert.config.hidden_size
        clip_hidden_size = self.clip.vision_model.config.hidden_size
        
        self.clip_projection = nn.Linear(clip_hidden_size, colbert_hidden_size, bias=False)
        
        # æœ€ç»ˆè¾“å‡ºå±‚ï¼šç»Ÿä¸€åˆ°æŒ‡å®šç»´åº¦
        self.output_projection = nn.Linear(colbert_hidden_size, output_dim, bias=False)
        
        print(f"  - ColBERTæ¨¡å‹: {colbert_model_name}")
        print(f"  - CLIPæ¨¡å‹: {clip_model_name}")
        print(f"  - è¾“å‡ºç»´åº¦: {output_dim}")

    def encode_text_with_colbert(self, texts):

        # ä½¿ç”¨ColBERTçš„åˆ†è¯å™¨
        inputs = self.colbert_tokenizer(
            texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )
        
        # é€šè¿‡ColBERTæ¨¡å‹
        with torch.no_grad():
            outputs = self.colbert(**inputs)
            # ColBERTä½¿ç”¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€
            text_embeddings = outputs.last_hidden_state
        
        return text_embeddings

    def encode_image_with_clip(self, images):
        # ä½¿ç”¨CLIPå¤„ç†å™¨
        inputs = self.clip_processor(images=images, return_tensors="pt", padding=True)
        
        # é€šè¿‡CLIPè§†è§‰æ¨¡å‹
        with torch.no_grad():
            vision_outputs = self.clip.vision_model(**inputs)
            patch_embeddings = vision_outputs.last_hidden_state  # (batch, num_patches+1, hidden)
            
            # å»æ‰CLS token
            patch_embeddings = patch_embeddings[:, 1:, :]  # (batch, num_patches, hidden)
        
        # æŠ•å½±åˆ°ColBERTç»´åº¦
        patch_embeddings = self.clip_projection(patch_embeddings)
        
        return patch_embeddings

    def compute_colbert_similarity(self, query_embeds, doc_embeds):

        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        sim_matrix = torch.matmul(query_embeds, doc_embeds.transpose(-2, -1))
        
        # ColBERTçš„MaxSimï¼šå¯¹æ¯ä¸ªæŸ¥è¯¢tokenï¼Œæ‰¾åˆ°æœ€ç›¸ä¼¼çš„æ–‡æ¡£token
        max_sim_per_query = sim_matrix.max(dim=-1).values  # (batch_size, query_len)
        
        # å¯¹æ‰€æœ‰æŸ¥è¯¢tokençš„MaxSimæ±‚å’Œ
        total_similarity = max_sim_per_query.sum(dim=-1)  # (batch_size,)
        
        return total_similarity

    def forward(self, query_texts, doc_images):
        # ä½¿ç”¨ColBERTç¼–ç æ–‡æœ¬
        query_embeds = self.encode_text_with_colbert(query_texts)
        
        # ä½¿ç”¨CLIPç¼–ç å›¾åƒï¼Œç„¶åæŠ•å½±åˆ°ColBERTç»´åº¦
        doc_embeds = self.encode_image_with_clip(doc_images)
        
        # è®¡ç®—ColBERTç›¸ä¼¼åº¦
        similarity = self.compute_colbert_similarity(query_embeds, doc_embeds)
        
        return similarity

    def retrieve_images(self, query_text, image_candidates, top_k=5):
        # ä½¿ç”¨ColBERTç¼–ç æŸ¥è¯¢æ–‡æœ¬
        query_embeds = self.encode_text_with_colbert([query_text])
        
        # è®¡ç®—æ‰€æœ‰å€™é€‰å›¾åƒçš„ç›¸ä¼¼åº¦
        all_scores = []
        for image in image_candidates:
            doc_embeds = self.encode_image_with_clip([image])
            score = self.compute_colbert_similarity(query_embeds, doc_embeds)
            all_scores.append(score.item())
        
        # æ’åºå¹¶è¿”å›top-kç»“æœ
        sorted_indices = torch.argsort(torch.tensor(all_scores), descending=True)
        top_indices = sorted_indices[:top_k]
        
        top_images = [image_candidates[i] for i in top_indices]
        top_scores = [all_scores[i] for i in top_indices]
        
        return top_images, top_scores

    def get_model_info(self):
        return {
            'colbert_model': self.colbert.config.name_or_path,
            'clip_model': self.clip.config.name_or_path,
            'colbert_hidden_size': self.colbert.config.hidden_size,
            'clip_hidden_size': self.clip.vision_model.config.hidden_size,
            'output_dim': self.output_projection.out_features
        }


path = "try_data"
    
# æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff']
image_files = []
    
for ext in image_extensions:
    image_files.extend(glob.glob(os.path.join(path, ext)))
    #image_files.extend(glob.glob(os.path.join(path, ext.upper())))
    
    
print(f"âœ… æ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡:")
for i, img_path in enumerate(image_files):
    print(f"  {i+1}. {os.path.basename(img_path)}")
    
model = VisionColBERT()
  
images = []
valid_images = []
    
for img_path in image_files:
    try:
        img = Image.open(img_path)
        # è½¬æ¢ä¸ºRGBæ¨¡å¼ï¼ˆå¤„ç†RGBAç­‰å…¶ä»–æ ¼å¼ï¼‰
        if img.mode != 'RGB':
            img = img.convert('RGB')
        images.append(img)
        valid_images.append(img_path)
        print(f"  âœ… {os.path.basename(img_path)} - å°ºå¯¸: {img.size}")
    except Exception as e:
        print(f"  âŒ {os.path.basename(img_path)} - åŠ è½½å¤±è´¥: {e}")
    
if not images:
    print("âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•å›¾ç‰‡")    
    
test_queries = [
        "bird",
        "English",
        "Chinese",
        "trees",
        "person"
    ]    
    
# æµ‹è¯•å›¾åƒæ£€ç´¢
for i, query in enumerate(test_queries[:3]):  # åªæµ‹è¯•å‰3ä¸ªæŸ¥è¯¢
    print(f"\næŸ¥è¯¢ {i+1}: '{query}'")
    try:
        top_images, top_scores = model.retrieve_images(query, images, top_k=3)
        print(f"  æ£€ç´¢ç»“æœ:")
        for j, (img, score) in enumerate(zip(top_images, top_scores)):
            img_index = images.index(img)
            img_name = os.path.basename(valid_images[img_index])
            print(f"    {j+1}. {img_name} - ç›¸ä¼¼åº¦: {score:.4f}")
    except Exception as e:
        print(f"  âŒ æ£€ç´¢å¤±è´¥: {e}")
    
# æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—
print(f"\nğŸ“Š æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—...")
try:
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªæŸ¥è¯¢å’Œæ‰€æœ‰å›¾åƒè®¡ç®—ç›¸ä¼¼åº¦
    query_embeds = model.encode_text_with_colbert([test_queries[0]])
    print(f"\næŸ¥è¯¢å†…å®¹ï¼š{test_queries[0]}")    
    similarities = []
    for i, img in enumerate(images):
        doc_embeds = model.encode_image_with_clip([img])
        similarity = model.compute_colbert_similarity(query_embeds, doc_embeds)
        similarities.append(similarity.item())
        img_name = os.path.basename(valid_images[i])
        print(f"  {img_name}: {similarity.item():.4f}")
        
    # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„å›¾åƒ
    max_idx = similarities.index(max(similarities))
    print(f"\nğŸ† æœ€ç›¸ä¼¼çš„å›¾åƒ: {os.path.basename(valid_images[max_idx])} (ç›¸ä¼¼åº¦: {max(similarities):.4f})")
        
except Exception as e:
    print(f"âŒ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
    
# æ¨¡å‹ä¿¡æ¯
print(f"\nğŸ“‹ æ¨¡å‹ä¿¡æ¯:")
model_info = model.get_model_info()
for key, value in model_info.items():
    print(f"  {key}: {value}")
    

 